{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80eb4366-bc44-4666-b526-721410e835d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c7b371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the folders and file paths\n",
    "papers_folder = 'F:/OAGProject/OAGFiles/OAGPublications'\n",
    "output_folder = 'F:/OAGProject/Output'\n",
    "references_folder = '/users/PGS0283/jhoope11/jack/Graphs/UpdatedFiles/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2364385d-b6db-44c3-a5bf-ec890856096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_papers(file_path):\n",
    "    print(f\"Reading papers from file: {file_path}\\n\")\n",
    "    papers = {}\n",
    "    with open(\"myfile.txt\", \"rbU\") as f:\n",
    "        num_lines = sum(1 for _ in f)\n",
    "    f.close()\n",
    "    expected_chunks = num_lines/2000\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        print(f\"number of lines: {num_lines}\\nexpected num of chunks: {expected_chunks}\")\n",
    "        for line in f:\n",
    "            paper_data = json.loads(line.strip())  # Parse each line as JSON\n",
    "            paper_id = paper_data.get(\"id\")\n",
    "            if not paper_id:\n",
    "                continue  # Skip this line if id is missing\n",
    "            title = paper_data.get(\"title\", \"\")\n",
    "            year = paper_data.get(\"year\", \"\")\n",
    "            venue_name = paper_data.get(\"venue\", \"\")\n",
    "            n_citation = paper_data.get(\"n_citation\", \"\")\n",
    "            authors = [{\"Name\": author.get(\"name\", \"\"), \"AuthorId\": author.get(\"id\", \"\"), \"AuthorOrg\": author.get(\"org\", \"\")} for author in paper_data.get(\"authors\", [])]\n",
    "            num_authors = len(authors)\n",
    "            keywords = paper_data.get(\"keywords\", \"\")\n",
    "            abstracts = paper_data.get(\"abstract\", \"\")\n",
    "            #fos = [field.get(\"name\", \"\") for field in paper_data.get(\"fos\", [])]\n",
    "            papers[paper_id] = {\n",
    "                \"Title\": title,\n",
    "                \"authors\": authors,\n",
    "                \"Year\": year,\n",
    "                \"Venue\": venue_name,\n",
    "                \"NumCitations\": n_citation,\n",
    "                \"NumAuthors\": num_authors,\n",
    "                \"Authors\": authors,\n",
    "                \"Keywords\": keywords,\n",
    "                \"Abstracts\": abstracts\n",
    "            }\n",
    "    print(\"Papers read.\\n\")\n",
    "    return papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361ba333-9c31-49b3-a117-6c2d070bb3c5",
   "metadata": {},
   "source": [
    "Need to break up the create_json_file function so the reference file can be made in its own function that is called on instead of at the end of the current function to attempt to fix the issue of only the last file be added to reference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19c272be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reference_file(output_folder, paper_id_mapping):\n",
    "    reference_file_path = os.path.join(output_folder, \"reference.json\")\n",
    "    with open(reference_file_path, 'w') as rf:\n",
    "        json.dump(paper_id_mapping, rf, indent=4)\n",
    "    print(f\"Reference file created: {reference_file_path}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb8839-b61f-4f8f-9e8c-1db8290c8f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json_file(output_folder, papers, master_file_name):\n",
    "    print(f\"Creating JSON files in folder: {output_folder}\\n\")\n",
    "    count = 0\n",
    "    file_count = 0\n",
    "    paper_id_mapping = {}\n",
    "\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    output_file = os.path.join(output_folder, f\"papers_chunk_{file_count}.json\")\n",
    "    with open(output_file, 'w') as f:\n",
    "        f.write(\"[\")\n",
    "        for paper_id, paper_info in papers.items():\n",
    "            if count == 2000:\n",
    "                f.write(\"\\n]\")\n",
    "                f.close()\n",
    "                # Map the current chunk to its corresponding file\n",
    "                paper_id_mapping.update({pid: f\"papers_chunk_{file_count}.json\" for pid in papers.keys()})\n",
    "                \n",
    "                # Start a new chunk\n",
    "                count = 0\n",
    "                file_count += 1\n",
    "                output_file = os.path.join(output_folder, f\"papers_chunk_{file_count}.json\")\n",
    "                f = open(output_file, 'w')\n",
    "                f.write(\"[\")\n",
    "            if count != 0:\n",
    "                f.write(\",\\n\")\n",
    "            f.write(json.dumps({\n",
    "                'PaperId': paper_id,\n",
    "                'PaperTitle': paper_info['Title'],\n",
    "                'MasterFileName': master_file_name,\n",
    "                'Journal': paper_info['Venue'],\n",
    "                'Year': paper_info['Year'],\n",
    "                'NumAuthors': paper_info['NumAuthors'],\n",
    "                'Authors': paper_info['Authors'],\n",
    "                'Keywords': paper_info['Keywords']\n",
    "            }))\n",
    "            count += 1\n",
    "        f.write(\"\\n]\")\n",
    "    \n",
    "    # Finalize mapping for the last chunk\n",
    "    paper_id_mapping.update({pid: f\"papers_chunk_{file_count}.json\" for pid in papers.keys()})\n",
    "\n",
    "    # Call function to create reference file\n",
    "    create_reference_file(output_folder, paper_id_mapping)\n",
    "\n",
    "    print(f\"JSON files created in folder: {output_folder}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af614af-9b71-43d8-a32c-fb505adb54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_papers(i, papers_folder):\n",
    "    print(f\"Processing papers file {i}\\n\")\n",
    "    papers_file = os.path.join(papers_folder, f\"v3.1_oag_publication_{i}.json\")\n",
    "    papers = read_papers(papers_file)\n",
    "\n",
    "    # for j in range(5):  # Assuming authors files are named mag_authors_0.txt to mag_authors_4.txt\n",
    "    #     print(f\"Processing authors file {j} for papers file {i}\\n\")\n",
    "    #     #authors_file = os.path.join(authors_folder, f\"mag_authors_{j}.txt\")\n",
    "    #     #read_authors(authors_file, affiliations, papers)\n",
    "\n",
    "    output_file = os.path.join(output_folder, f\"papers_chunk_{i}.json\")\n",
    "    create_json_file(output_file, papers, f\"v3.1_oag_publication_{i}.json\")\n",
    "    print(f\"Chunk {i} processing complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ffd02695-6b0e-419d-8fe7-0b5e01fc2a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_files(papers_folder, output_folder):\n",
    "    print(\"Processing files...\\n\")\n",
    "    \n",
    "    # with Pool(processes=28) as pool:\n",
    "    #     pool.starmap(process_papers, [(i, papers_folder) for i in range(5)])\n",
    "    # pool.close()\n",
    "    # pool.join()\n",
    "    process_papers(1, papers_folder)\n",
    "    print(\"All files processed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d449726-fae3-47b0-a9f4-d8f5e9274aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files...\n",
      "\n",
      "Processing papers file 1\n",
      "\n",
      "Reading papers from file: F:/OAGProject/OAGFiles/OAGPublications\\v3.1_oag_publication_1.json\n",
      "\n",
      "Papers read.\n",
      "\n",
      "Creating JSON files in folder: F:/OAGProject/Output\\papers_chunk_1.json\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Process all files\n",
    "process_files(papers_folder, output_folder)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
